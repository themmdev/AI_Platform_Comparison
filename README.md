# AI_Platform_Comparison
Comparative Study of AI Platforms: ChatGPT, Gemini, Grok, and Claude
ðŸ“Œ Project Overview

This repository contains the practical implementation and data for my final project in the Information and Communication Technologies (ICT) course. The project focuses on the "Artificial Intelligence Platforms" theme, specifically a comparative study of four leading Large Language Models (LLMs).

The goal of this research is to evaluate and rank the performance of ChatGPT (OpenAI), Gemini (Google), Grok (xAI), and Claude (Anthropic) across five critical performance dimensions.
ðŸŽ¯ Research Objectives

    To assess the accuracy and logic of AI models in STEM and reasoning tasks.

To evaluate the technical proficiency and safety of generated code.

To analyze the creative nuance and factual reliability of each platform.

To identify ethical risks and alignment differences between the models.

ðŸ›  ICT Tools Used

As per the project requirements, the following tools were utilized:

    GitHub: For version control, project documentation, and hosting this experimental log.

Google Scholar: For conducting the literature review and sourcing academic references.

Microsoft Excel: For data recording, scoring, and generating visual analytics.

ðŸ§ª Methodology & Evaluation Criteria

The platforms were tested using 15 standardized prompts categorized into the following five areas:

    Logical Reasoning & STEM: Testing step-by-step problem solving.

    Coding & Technical: Evaluating script generation and security analysis.

    Creative Writing & Tone: Testing stylistic flexibility and summarization.

    Factuality & Real-Time Data: Checking for hallucinations and current event awareness.

    Safety & Ethics: Assessing the handling of controversial or sensitive topics.

ðŸ“‚ Repository Structure
Plaintext

AI_Platform_Comparison/
â”œâ”€â”€ README.md (overview and methodology)
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ stem_prompts.txt
â”‚   â”œâ”€â”€ coding_prompts.txt
â”‚   â”œâ”€â”€ creative_prompts.txt
â”‚   â”œâ”€â”€ factuality_prompts.txt
â”‚   â””â”€â”€ ethics_prompts.txt
â”œâ”€â”€ responses/
â”‚   â”œâ”€â”€ chatgpt_responses.txt
â”‚   â”œâ”€â”€ gemini_responses.txt
â”‚   â”œâ”€â”€ claude_responses.txt
â”‚   â””â”€â”€ grok_responses.txt
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ scoring_matrix.xlsx
â”‚   â””â”€â”€ analysis_notes.md
â””â”€â”€ documentation/
    â”œâ”€â”€ methodology.md
    â””â”€â”€ test_log.md

ðŸ“Š Evaluation Rubric

Each model was scored on a scale of 1â€“5 for every prompt based on:

    Accuracy: How correct the information is.

    Completeness: Whether all parts of the prompt were addressed.

    Clarity: How easy the response is to understand.

ðŸ“œ Academic Honesty & Transparency

    Plagiarism: This project adheres to the <15% plagiarism threshold.

AI Disclosure: AI tools were used for assistance in drafting test prompts and structuring data. A full log of prompts and reflections is included in the final report appendix.

Author: [Your Name]

Course: Information and Communication Technologies (ICT)

Date: December 2024
