# AI_Platform_Comparison
Comparative Study of AI Platforms: ChatGPT, Gemini, Grok, and Claude
ðŸ“Œ Project Overview

This repository contains the practical implementation and data for my final project in the Information and Communication Technologies (ICT) course. The project focuses on the "Artificial Intelligence Platforms" theme, specifically a comparative study of four leading Large Language Models (LLMs).

The goal of this research is to evaluate and rank the performance of ChatGPT (OpenAI), Gemini (Google), Grok (xAI), and Claude (Anthropic) across five critical performance dimensions.
ðŸŽ¯ Research Objectives

    To assess the accuracy and logic of AI models in STEM and reasoning tasks.

To evaluate the technical proficiency and safety of generated code.

To analyze the creative nuance and factual reliability of each platform.

To identify ethical risks and alignment differences between the models.

ðŸ›  ICT Tools Used

As per the project requirements, the following tools were utilized:

    GitHub: For version control, project documentation, and hosting this experimental log.

Google Scholar: For conducting the literature review and sourcing academic references.

Microsoft Excel: For data recording, scoring, and generating visual analytics.

ðŸ§ª Methodology & Evaluation Criteria

The platforms were tested using 15 standardized prompts categorized into the following five areas:

    Logical Reasoning & STEM: Testing step-by-step problem solving.

    Coding & Technical: Evaluating script generation and security analysis.

    Creative Writing & Tone: Testing stylistic flexibility and summarization.

    Factuality & Real-Time Data: Checking for hallucinations and current event awareness.

    Safety & Ethics: Assessing the handling of controversial or sensitive topics.

ðŸ“‚ Repository Structure
Plaintext

AI_Platform_Comparison/
â”œâ”€â”€ README.md                # Main project overview, research goals, and methodology summary.
â”œâ”€â”€ prompts/                 # Standardized input data used as control variables for all AI models.
â”‚   â”œâ”€â”€ stem_prompts.txt     # Logic and mathematical reasoning test cases for performance evaluation.
â”‚   â”œâ”€â”€ coding_prompts.txt   # Technical tasks focusing on code generation and security auditing.
â”‚   â”œâ”€â”€ creative_prompts.txt # Stylistic writing and summarization tests for linguistic nuance.
â”‚   â”œâ”€â”€ factuality_prompts.txt# Verification tasks to check for real-time data accuracy and hallucinations.
â”‚   â””â”€â”€ ethics_prompts.txt   # Assessment of safety guardrails and algorithmic bias across platforms.
â”œâ”€â”€ responses/               # Raw experimental data serving as primary evidence for the report.
â”‚   â”œâ”€â”€ chatgpt_responses.txt# Unedited outputs from OpenAI GPT-4o for cross-model analysis.
â”‚   â”œâ”€â”€ gemini_responses.txt # Unedited outputs from Google Gemini Pro used in comparative scoring.
â”‚   â”œâ”€â”€ claude_responses.txt # Unedited outputs from Anthropic Claude 3.5 for evaluation.
â”‚   â””â”€â”€ grok_responses.txt   # Unedited outputs from xAI Grok for the performance benchmarking.
â”œâ”€â”€ evaluation/              # Data processing and quantitative analysis of the AI outputs.
â”‚   â”œâ”€â”€ scoring_matrix.xlsx  # Excel-based comparison tool for generating charts and statistical data.
â”‚   â””â”€â”€ analysis_notes.md    # Qualitative observations and critical reflections on model performance.
â””â”€â”€ documentation/           # Supporting files for report structure and academic transparency.
    â”œâ”€â”€ methodology.md       # Detailed explanation of experimental design and tool selection.
    â””â”€â”€ test_log.md          # Timestamped record of test execution for transparency and reproducibility.

ðŸ“Š Evaluation Rubric

Each model was scored on a scale of 1â€“5 for every prompt based on:

    Accuracy: How correct the information is.

    Completeness: Whether all parts of the prompt were addressed.

    Clarity: How easy the response is to understand.

ðŸ“œ Academic Honesty & Transparency

    Plagiarism: This project adheres to the <15% plagiarism threshold.

AI Disclosure: AI tools were used for assistance in drafting test prompts and structuring data. A full log of prompts and reflections is included in the final report appendix.

Author: [Your Name]

Course: Information and Communication Technologies (ICT)

Date: December 2024
